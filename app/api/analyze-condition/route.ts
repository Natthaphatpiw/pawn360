import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || '',
});

// Agent 3: Analyze condition from images
async function analyzeConditionFromImages(images: string[], userCondition?: number): Promise<{ score: number; reason: string }> {
  if (images.length === 0) {
    // If no images, use user-reported condition (convert to 0-1 scale)
    const score = (userCondition || 50) / 100;
    return {
      score,
      reason: `‡∏™‡∏†‡∏≤‡∏û‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏µ‡πà ${Math.round(score * 100)}% ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏∏ (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö)`
    };
  }

  // Use OpenAI Vision to analyze condition from images
  const prompt = `‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å 0.0 ‡∏ñ‡∏∂‡∏á 1.0 ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏±‡πâ‡∏ô‡πÜ

‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏†‡∏≤‡∏û:
- 1.0 = ‡∏™‡∏†‡∏≤‡∏û‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- 0.8 = ‡∏™‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏°‡∏≤‡∏Å ‡∏°‡∏µ‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏Å‡∏ï‡∏¥
- 0.6 = ‡∏™‡∏†‡∏≤‡∏û‡∏î‡∏µ ‡∏°‡∏µ‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏õ‡∏Å‡∏ï‡∏¥
- 0.4 = ‡∏™‡∏†‡∏≤‡∏û‡∏û‡∏≠‡πÉ‡∏ä‡πâ ‡∏°‡∏µ‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏ï‡∏≥‡∏´‡∏ô‡∏¥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î
- 0.2 = ‡∏™‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πà‡∏î‡∏µ ‡∏°‡∏µ‡∏ï‡∏≥‡∏´‡∏ô‡∏¥‡πÅ‡∏•‡∏∞‡∏£‡πà‡∏≠‡∏á‡∏£‡∏≠‡∏¢‡∏°‡∏≤‡∏Å
- 0.0 = ‡∏™‡∏†‡∏≤‡∏û‡πÄ‡∏•‡∏ß‡∏°‡∏≤‡∏Å ‡πÅ‡∏ó‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ

‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á (‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏™‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ï‡∏ô‡πÄ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á)

‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON:
{
  "score": 0.85,
  "reason": "‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏°‡∏≤‡∏Å..."
}`;

  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini', // Use vision-capable model
      messages: [
        {
          role: 'user',
          content: [
            { type: 'text', text: prompt },
            // Note: In production, you would include image URLs here
            // For now, we'll simulate with text-based analysis
          ]
        }
      ],
      max_tokens: 200,
      temperature: 0.2,
    });

    const content = response.choices[0]?.message?.content || '';

    // Try to parse JSON response
    try {
      const parsed = JSON.parse(content);
      return {
        score: Math.max(0, Math.min(1, parsed.score || 0.5)),
        reason: parsed.reason || '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ'
      };
    } catch {
      // If JSON parsing fails, extract score and reason from text
      const scoreMatch = content.match(/score["\s:]+([0-9.]+)/i);
      const score = scoreMatch ? parseFloat(scoreMatch[1]) : 0.5;

      return {
        score: Math.max(0, Math.min(1, score)),
        reason: content.replace(/score["\s:]+[0-9.]+/i, '').trim() || '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÅ‡∏•‡πâ‡∏ß'
      };
    }
  } catch (error) {
    console.error('Error analyzing condition:', error);
    // Fallback to mock analysis
    return {
      score: 0.5,
      reason: '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô'
    };
  }
}

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { images, userCondition } = body;

    if (!images || !Array.isArray(images) || images.length === 0) {
      return NextResponse.json({ error: '‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏£‡∏π‡∏õ' }, { status: 400 });
    }

    if (!process.env.OPENAI_API_KEY) {
      return NextResponse.json(
        { error: 'OpenAI API key not configured' },
        { status: 500 }
      );
    }

    console.log('üîÑ Analyzing condition from images...');
    const conditionResult = await analyzeConditionFromImages(images, userCondition);
    console.log('‚úÖ Condition analysis complete:', conditionResult);

    return NextResponse.json(conditionResult);
  } catch (error) {
    console.error('Error analyzing condition:', error);
    return NextResponse.json({ error: '‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏†‡∏≤‡∏û' }, { status: 500 });
  }
}
